{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:04.271266730Z",
     "start_time": "2023-10-06T08:13:03.126101534Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Тензоры в pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:04.783748891Z",
     "start_time": "2023-10-06T08:13:03.169846125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5.]),\n",
       " tensor([5., 6., 7., 8., 9.]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3.0,4,5], dtype=torch.float32)\n",
    "b = torch.arange(5, 10, dtype=torch.float32)\n",
    "c = torch.ones(5, 4, dtype=torch.float32)\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:04.784631204Z",
     "start_time": "2023-10-06T08:13:03.207845933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Над тензорами в pytorch можно производить все матиматические операции как в numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:04.861946769Z",
     "start_time": "2023-10-06T08:13:03.248128105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\ttensor([1., 2., 3., 4., 5.])\n",
      "b\ttensor([5., 6., 7., 8., 9.])\n",
      "a * b\ttensor([ 5., 12., 21., 32., 45.])\n",
      "a + b\ttensor([ 6.,  8., 10., 12., 14.])\n",
      "a - b\ttensor([-4., -4., -4., -4., -4.])\n",
      "a / b\ttensor([0.2000, 0.3333, 0.4286, 0.5000, 0.5556])\n",
      "a//b\ttensor([0., 0., 0., 0., 0.])\n",
      "a**b\ttensor([1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04, 1.9531e+06])\n",
      "a > b\ttensor([False, False, False, False, False])\n",
      "a==b\ttensor([False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "operation_names = ['a','b', 'a * b', 'a + b', 'a - b', 'a / b', 'a//b', 'a**b', 'a > b', 'a==b']\n",
    "operations = [a, b, a * b, a + b, a - b, a / b, a//b, a**b, a > b, a==b]\n",
    "for name, result in zip(operation_names, operations):\n",
    "    print(name, result, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно работать и с обычными числами, например так, используя специализированные функции pytorch для сложения, умножения, вычитания и т.д.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:04.863639624Z",
     "start_time": "2023-10-06T08:13:03.248998965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(2.), tensor(-1.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1.\n",
    "y = 2.\n",
    "torch.add(x, y), torch.mul(x,y), torch.sub(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако для простоты записи, удобнее сразу перевести все переменные в pytorch тензоры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:05.026916457Z",
     "start_time": "2023-10-06T08:13:03.283418891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(2.), tensor(-1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.)\n",
    "y = torch.tensor(2.)\n",
    "torch.add(x, y), torch.mul(x,y), torch.sub(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "тогда вместо pytorch методов можно пользоваться переопределёнными обычными математическими операциями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:05.514764906Z",
     "start_time": "2023-10-06T08:13:03.333383055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(2.), tensor(-1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y, x * y, x - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Автоматическое дифференцирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.066410661Z",
     "start_time": "2023-10-06T08:13:03.349340644Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(15., requires_grad=True) # =requires_grad=True => PyTorch будет автоматически вычислять градиенты\n",
    "y = torch.tensor(24.)\n",
    "f = torch.log(x**2 + y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.680139967Z",
     "start_time": "2023-10-06T08:13:03.384161642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6859, grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.688826769Z",
     "start_time": "2023-10-06T08:13:03.444049970Z"
    }
   },
   "outputs": [],
   "source": [
    "# u = torch.exp(f)\n",
    "# u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.690103373Z",
     "start_time": "2023-10-06T08:13:03.446907625Z"
    }
   },
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.692075510Z",
     "start_time": "2023-10-06T08:13:03.448516555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15., requires_grad=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.741546547Z",
     "start_time": "2023-10-06T08:13:03.456721804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0375)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.798278198Z",
     "start_time": "2023-10-06T08:13:03.472042632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.799148024Z",
     "start_time": "2023-10-06T08:13:03.516002503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.809759328Z",
     "start_time": "2023-10-06T08:13:03.516610153Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tan(1 + x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.814189960Z",
     "start_time": "2023-10-06T08:13:03.559836165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.1850, grad_fn=<TanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.867520290Z",
     "start_time": "2023-10-06T08:13:03.561028522Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.868507492Z",
     "start_time": "2023-10-06T08:13:03.572881025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.5488)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.878129733Z",
     "start_time": "2023-10-06T08:13:03.616015958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.548798408083835"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = 1\n",
    "2*x / np.cos(1+x**2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Более сложный пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.902829270Z",
     "start_time": "2023-10-06T08:13:03.616497269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6904, 0.7417, 0.6027, 0.7612],\n",
       "         [0.3263, 0.9509, 0.6202, 0.9371],\n",
       "         [0.1982, 0.1809, 0.3788, 0.1230]]),\n",
       " tensor([0., 1., 2., 3.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.rand(3, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "W, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.903762320Z",
     "start_time": "2023-10-06T08:13:03.616949461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6904, 0.7417, 0.6027, 0.7612],\n",
       "        [0.3263, 0.9509, 0.6202, 0.9371],\n",
       "        [0.1982, 0.1809, 0.3788, 0.1230]], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.905275024Z",
     "start_time": "2023-10-06T08:13:03.659752171Z"
    }
   },
   "outputs": [],
   "source": [
    "#z = W.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike NumPy’s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.906090982Z",
     "start_time": "2023-10-06T08:13:03.660348436Z"
    }
   },
   "outputs": [],
   "source": [
    "z = W.matmul(x)\n",
    "y = torch.relu(z)   # Вычисляется y, применяя функцию ReLU (Rectified Linear Unit) к z. \n",
    "                    # Это популярная нелинейная активационная функция, которая отбрасывает отрицательные значения.\n",
    "target = torch.tensor([1., 1.2, 1.4])\n",
    "loss = torch.sum((y - target)**2)   # Вычисляется функция потерь loss. В данном случае, это средняя квадратичная ошибка \n",
    "                                    # между y и target. То есть, (y - target)^2 и затем суммируется вся ошибка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.912560153Z",
     "start_time": "2023-10-06T08:13:03.660898828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.9044, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.941056944Z",
     "start_time": "2023-10-06T08:13:03.672659189Z"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward() # Вызывается loss.backward(), что позволяет PyTorch автоматически вычислить градиенты параметров \n",
    "                # (в данном случае, параметры W) относительно функции потерь loss с использованием метода обратного \n",
    "                # распространения ошибки (backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:06.942589356Z",
     "start_time": "2023-10-06T08:13:03.715930488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  6.4615, 12.9230, 19.3845],\n",
       "        [ 0.0000,  7.6048, 15.2095, 22.8143],\n",
       "        [-0.0000, -0.1850, -0.3699, -0.5549]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.007220832Z",
     "start_time": "2023-10-06T08:13:03.716587736Z"
    }
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.014038351Z",
     "start_time": "2023-10-06T08:13:03.759480826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 33.65700149536133\n",
      "iteration 1, loss 4.400000095367432\n",
      "iteration 2, loss 4.400000095367432\n",
      "iteration 3, loss 4.400000095367432\n",
      "iteration 4, loss 4.400000095367432\n",
      "iteration 5, loss 4.400000095367432\n",
      "iteration 6, loss 4.400000095367432\n",
      "iteration 7, loss 4.400000095367432\n",
      "iteration 8, loss 4.400000095367432\n",
      "iteration 9, loss 4.400000095367432\n"
     ]
    }
   ],
   "source": [
    "W = torch.rand(3, 4, requires_grad=True)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "target = torch.tensor([1., 1.2, 1.4])\n",
    "for i in range(10):\n",
    "    z = W.matmul(x)\n",
    "    y = torch.relu(z)\n",
    "    loss = torch.sum((y - target)**2)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():   # параметры W обновляются в соответствии с градиентами с использованием стохастического \n",
    "         W -= 0.1 * W.grad  # градиентного спуска (SGD). Обновление происходит в блоке with torch.no_grad(),\n",
    "                            # чтобы избежать создания дополнительных вычислений градиентов.\n",
    "    W.grad.zero_()          # градиенты обнуляются после каждой итерации с W.grad.zero(). Это важно, потому что PyTorch \n",
    "                            # автоматически аккумулирует градиенты, и вы хотите начать с чистого листа на каждой итерации.\n",
    "    print(f'iteration {i}, loss {loss}')\n",
    "    #print(f'grad {W.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.031255059Z",
     "start_time": "2023-10-06T08:13:03.760102488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x 4.0, loss 6.25, grad 5.0\n",
      "iteration 1, x 3.5, loss 4.0, grad 4.0\n",
      "iteration 2, x 3.0999999046325684, loss 2.559999704360962, grad 3.1999998092651367\n",
      "iteration 3, x 2.7799999713897705, loss 1.6383999586105347, grad 2.559999942779541\n",
      "iteration 4, x 2.5239999294281006, loss 1.0485758781433105, grad 2.047999858856201\n",
      "iteration 5, x 2.319200038909912, loss 0.671088695526123, grad 1.6384000778198242\n",
      "iteration 6, x 2.155359983444214, loss 0.429496705532074, grad 1.3107199668884277\n",
      "iteration 7, x 2.0242879390716553, loss 0.2748778462409973, grad 1.0485758781433105\n",
      "iteration 8, x 1.9194303750991821, loss 0.17592184245586395, grad 0.8388607501983643\n",
      "iteration 9, x 1.8355443477630615, loss 0.1125900074839592, grad 0.671088695526123\n",
      "iteration 10, x 1.7684354782104492, loss 0.07205760478973389, grad 0.5368709564208984\n",
      "iteration 11, x 1.7147483825683594, loss 0.046116866171360016, grad 0.42949676513671875\n",
      "iteration 12, x 1.6717987060546875, loss 0.029514795169234276, grad 0.343597412109375\n",
      "iteration 13, x 1.6374390125274658, loss 0.01888948306441307, grad 0.27487802505493164\n",
      "iteration 14, x 1.6099512577056885, loss 0.012089279480278492, grad 0.21990251541137695\n",
      "iteration 15, x 1.587960958480835, loss 0.007737130392342806, grad 0.17592191696166992\n",
      "iteration 16, x 1.570368766784668, loss 0.004951763432472944, grad 0.14073753356933594\n",
      "iteration 17, x 1.5562950372695923, loss 0.0031691312324255705, grad 0.11259007453918457\n",
      "iteration 18, x 1.5450360774993896, loss 0.0020282482728362083, grad 0.0900721549987793\n",
      "iteration 19, x 1.5360288619995117, loss 0.0012980789178982377, grad 0.07205772399902344\n",
      "iteration 20, x 1.5288231372833252, loss 0.000830773264169693, grad 0.05764627456665039\n",
      "iteration 21, x 1.523058533668518, loss 0.0005316959577612579, grad 0.04611706733703613\n",
      "iteration 22, x 1.5184468030929565, loss 0.00034028454683721066, grad 0.036893606185913086\n",
      "iteration 23, x 1.5147573947906494, loss 0.00021778070367872715, grad 0.029514789581298828\n",
      "iteration 24, x 1.5118058919906616, loss 0.00013937908806838095, grad 0.023611783981323242\n",
      "iteration 25, x 1.5094447135925293, loss 8.920261461753398e-05, grad 0.018889427185058594\n",
      "iteration 26, x 1.5075557231903076, loss 5.708895332645625e-05, grad 0.015111446380615234\n",
      "iteration 27, x 1.506044626235962, loss 3.6537505366140977e-05, grad 0.012089252471923828\n",
      "iteration 28, x 1.5048357248306274, loss 2.338423473702278e-05, grad 0.009671449661254883\n",
      "iteration 29, x 1.503868579864502, loss 1.4965909940656275e-05, grad 0.007737159729003906\n",
      "iteration 30, x 1.5030949115753174, loss 9.578478056937456e-06, grad 0.006189823150634766\n",
      "iteration 31, x 1.5024759769439697, loss 6.130461770226248e-06, grad 0.004951953887939453\n",
      "iteration 32, x 1.5019807815551758, loss 3.9234955693245865e-06, grad 0.0039615631103515625\n",
      "iteration 33, x 1.5015846490859985, loss 2.511112825231976e-06, grad 0.0031692981719970703\n",
      "iteration 34, x 1.501267671585083, loss 1.6069911907834467e-06, grad 0.0025353431701660156\n",
      "iteration 35, x 1.5010141134262085, loss 1.0284260270054801e-06, grad 0.002028226852416992\n",
      "iteration 36, x 1.5008113384246826, loss 6.582700393664709e-07, grad 0.0016226768493652344\n",
      "iteration 37, x 1.500649094581604, loss 4.2132376165682217e-07, grad 0.0012981891632080078\n",
      "iteration 38, x 1.5005192756652832, loss 2.696472165553132e-07, grad 0.0010385513305664062\n",
      "iteration 39, x 1.5004154443740845, loss 1.7259402795843926e-07, grad 0.0008308887481689453\n",
      "iteration 40, x 1.5003323554992676, loss 1.1046017789340112e-07, grad 0.0006647109985351562\n",
      "iteration 41, x 1.5002658367156982, loss 7.066915941322804e-08, grad 0.0005316734313964844\n",
      "iteration 42, x 1.5002126693725586, loss 4.522826202446595e-08, grad 0.0004253387451171875\n",
      "iteration 43, x 1.500170111656189, loss 2.893797557135258e-08, grad 0.0003402233123779297\n",
      "iteration 44, x 1.500136137008667, loss 1.8533285128796706e-08, grad 0.0002722740173339844\n",
      "iteration 45, x 1.5001089572906494, loss 1.1871691185660893e-08, grad 0.00021791458129882812\n",
      "iteration 46, x 1.5000871419906616, loss 7.593726536470058e-09, grad 0.0001742839813232422\n",
      "iteration 47, x 1.5000697374343872, loss 4.863309754910006e-09, grad 0.00013947486877441406\n",
      "iteration 48, x 1.5000557899475098, loss 3.1125182431424037e-09, grad 0.00011157989501953125\n",
      "iteration 49, x 1.500044584274292, loss 1.9877575141435955e-09, grad 8.916854858398438e-05\n",
      "iteration 50, x 1.5000356435775757, loss 1.2704646223937743e-09, grad 7.128715515136719e-05\n",
      "iteration 51, x 1.5000284910202026, loss 8.117382321870537e-10, grad 5.698204040527344e-05\n",
      "iteration 52, x 1.5000227689743042, loss 5.184261908652843e-10, grad 4.553794860839844e-05\n",
      "iteration 53, x 1.5000182390213013, loss 3.326618980281637e-10, grad 3.647804260253906e-05\n",
      "iteration 54, x 1.5000145435333252, loss 2.1151436158106662e-10, grad 2.9087066650390625e-05\n",
      "iteration 55, x 1.500011682510376, loss 1.3648104868480004e-10, grad 2.3365020751953125e-05\n",
      "iteration 56, x 1.500009298324585, loss 8.645884008728899e-11, grad 1.8596649169921875e-05\n",
      "iteration 57, x 1.5000073909759521, loss 5.46265255252365e-11, grad 1.4781951904296875e-05\n",
      "iteration 58, x 1.5000059604644775, loss 3.552713678800501e-11, grad 1.1920928955078125e-05\n",
      "iteration 59, x 1.500004768371582, loss 2.2737367544323206e-11, grad 9.5367431640625e-06\n",
      "iteration 60, x 1.5000038146972656, loss 1.4551915228366852e-11, grad 7.62939453125e-06\n",
      "iteration 61, x 1.5000030994415283, loss 9.606537787476555e-12, grad 6.198883056640625e-06\n",
      "iteration 62, x 1.5000025033950806, loss 6.266986929404084e-12, grad 5.0067901611328125e-06\n",
      "iteration 63, x 1.5000020265579224, loss 4.106937012693379e-12, grad 4.0531158447265625e-06\n",
      "iteration 64, x 1.5000016689300537, loss 2.7853275241795927e-12, grad 3.337860107421875e-06\n",
      "iteration 65, x 1.500001311302185, loss 1.7195134205394424e-12, grad 2.6226043701171875e-06\n",
      "iteration 66, x 1.500001072883606, loss 1.1510792319313623e-12, grad 2.1457672119140625e-06\n",
      "iteration 67, x 1.5000008344650269, loss 6.963318810448982e-13, grad 1.6689300537109375e-06\n",
      "iteration 68, x 1.5000007152557373, loss 5.115907697472721e-13, grad 1.430511474609375e-06\n",
      "iteration 69, x 1.5000005960464478, loss 3.552713678800501e-13, grad 1.1920928955078125e-06\n",
      "iteration 70, x 1.5000004768371582, loss 2.2737367544323206e-13, grad 9.5367431640625e-07\n",
      "iteration 71, x 1.5000003576278687, loss 1.2789769243681803e-13, grad 7.152557373046875e-07\n",
      "iteration 72, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 73, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 74, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 75, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 76, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 77, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 78, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 79, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 80, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 81, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 82, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 83, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 84, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 85, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 86, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 87, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 88, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 89, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 90, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 91, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 92, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 93, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 94, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 95, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 96, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 97, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 98, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n",
      "iteration 99, x 1.500000238418579, loss 5.684341886080802e-14, grad 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "'''Этот код выполняет градиентный спуск для минимизации функции потерь f(x). \n",
    "В каждой итерации значение x обновляется в направлении, которое уменьшает \n",
    "значение функции потерь, и градиенты пересчитываются. Этот процесс повторяется \n",
    "100 раз, и x сходится к значению, которое минимизирует функцию потерь.'''\n",
    "\n",
    "def f(x):\n",
    "    return (x - 1.5)**2\n",
    "\n",
    "x = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "for i in range(100):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    print(f'iteration {i}, x {x}, loss {y}, grad {x.grad}')\n",
    "    with torch.no_grad():\n",
    "        x -= 0.1 * x.grad\n",
    "    x.grad.zero_()\n",
    "    #print(f'grad {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.101984304Z",
     "start_time": "2023-10-06T08:13:03.823640556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x tensor([3.5000], requires_grad=True), loss tensor([6.2500], grad_fn=<PowBackward0>), grad tensor([5.])\n",
      "iteration 1, x tensor([3.1000], requires_grad=True), loss tensor([4.], grad_fn=<PowBackward0>), grad tensor([4.])\n",
      "iteration 2, x tensor([2.7800], requires_grad=True), loss tensor([2.5600], grad_fn=<PowBackward0>), grad tensor([3.2000])\n",
      "iteration 3, x tensor([2.5240], requires_grad=True), loss tensor([1.6384], grad_fn=<PowBackward0>), grad tensor([2.5600])\n",
      "iteration 4, x tensor([2.3192], requires_grad=True), loss tensor([1.0486], grad_fn=<PowBackward0>), grad tensor([2.0480])\n",
      "iteration 5, x tensor([2.1554], requires_grad=True), loss tensor([0.6711], grad_fn=<PowBackward0>), grad tensor([1.6384])\n",
      "iteration 6, x tensor([2.0243], requires_grad=True), loss tensor([0.4295], grad_fn=<PowBackward0>), grad tensor([1.3107])\n",
      "iteration 7, x tensor([1.9194], requires_grad=True), loss tensor([0.2749], grad_fn=<PowBackward0>), grad tensor([1.0486])\n",
      "iteration 8, x tensor([1.8355], requires_grad=True), loss tensor([0.1759], grad_fn=<PowBackward0>), grad tensor([0.8389])\n",
      "iteration 9, x tensor([1.7684], requires_grad=True), loss tensor([0.1126], grad_fn=<PowBackward0>), grad tensor([0.6711])\n",
      "iteration 10, x tensor([1.7147], requires_grad=True), loss tensor([0.0721], grad_fn=<PowBackward0>), grad tensor([0.5369])\n",
      "iteration 11, x tensor([1.6718], requires_grad=True), loss tensor([0.0461], grad_fn=<PowBackward0>), grad tensor([0.4295])\n",
      "iteration 12, x tensor([1.6374], requires_grad=True), loss tensor([0.0295], grad_fn=<PowBackward0>), grad tensor([0.3436])\n",
      "iteration 13, x tensor([1.6100], requires_grad=True), loss tensor([0.0189], grad_fn=<PowBackward0>), grad tensor([0.2749])\n",
      "iteration 14, x tensor([1.5880], requires_grad=True), loss tensor([0.0121], grad_fn=<PowBackward0>), grad tensor([0.2199])\n",
      "iteration 15, x tensor([1.5704], requires_grad=True), loss tensor([0.0077], grad_fn=<PowBackward0>), grad tensor([0.1759])\n",
      "iteration 16, x tensor([1.5563], requires_grad=True), loss tensor([0.0050], grad_fn=<PowBackward0>), grad tensor([0.1407])\n",
      "iteration 17, x tensor([1.5450], requires_grad=True), loss tensor([0.0032], grad_fn=<PowBackward0>), grad tensor([0.1126])\n",
      "iteration 18, x tensor([1.5360], requires_grad=True), loss tensor([0.0020], grad_fn=<PowBackward0>), grad tensor([0.0901])\n",
      "iteration 19, x tensor([1.5288], requires_grad=True), loss tensor([0.0013], grad_fn=<PowBackward0>), grad tensor([0.0721])\n",
      "iteration 20, x tensor([1.5231], requires_grad=True), loss tensor([0.0008], grad_fn=<PowBackward0>), grad tensor([0.0576])\n",
      "iteration 21, x tensor([1.5184], requires_grad=True), loss tensor([0.0005], grad_fn=<PowBackward0>), grad tensor([0.0461])\n",
      "iteration 22, x tensor([1.5148], requires_grad=True), loss tensor([0.0003], grad_fn=<PowBackward0>), grad tensor([0.0369])\n",
      "iteration 23, x tensor([1.5118], requires_grad=True), loss tensor([0.0002], grad_fn=<PowBackward0>), grad tensor([0.0295])\n",
      "iteration 24, x tensor([1.5094], requires_grad=True), loss tensor([0.0001], grad_fn=<PowBackward0>), grad tensor([0.0236])\n",
      "iteration 25, x tensor([1.5076], requires_grad=True), loss tensor([8.9203e-05], grad_fn=<PowBackward0>), grad tensor([0.0189])\n",
      "iteration 26, x tensor([1.5060], requires_grad=True), loss tensor([5.7089e-05], grad_fn=<PowBackward0>), grad tensor([0.0151])\n",
      "iteration 27, x tensor([1.5048], requires_grad=True), loss tensor([3.6538e-05], grad_fn=<PowBackward0>), grad tensor([0.0121])\n",
      "iteration 28, x tensor([1.5039], requires_grad=True), loss tensor([2.3384e-05], grad_fn=<PowBackward0>), grad tensor([0.0097])\n",
      "iteration 29, x tensor([1.5031], requires_grad=True), loss tensor([1.4966e-05], grad_fn=<PowBackward0>), grad tensor([0.0077])\n",
      "iteration 30, x tensor([1.5025], requires_grad=True), loss tensor([9.5785e-06], grad_fn=<PowBackward0>), grad tensor([0.0062])\n",
      "iteration 31, x tensor([1.5020], requires_grad=True), loss tensor([6.1305e-06], grad_fn=<PowBackward0>), grad tensor([0.0050])\n",
      "iteration 32, x tensor([1.5016], requires_grad=True), loss tensor([3.9235e-06], grad_fn=<PowBackward0>), grad tensor([0.0040])\n",
      "iteration 33, x tensor([1.5013], requires_grad=True), loss tensor([2.5111e-06], grad_fn=<PowBackward0>), grad tensor([0.0032])\n",
      "iteration 34, x tensor([1.5010], requires_grad=True), loss tensor([1.6070e-06], grad_fn=<PowBackward0>), grad tensor([0.0025])\n",
      "iteration 35, x tensor([1.5008], requires_grad=True), loss tensor([1.0284e-06], grad_fn=<PowBackward0>), grad tensor([0.0020])\n",
      "iteration 36, x tensor([1.5006], requires_grad=True), loss tensor([6.5827e-07], grad_fn=<PowBackward0>), grad tensor([0.0016])\n",
      "iteration 37, x tensor([1.5005], requires_grad=True), loss tensor([4.2132e-07], grad_fn=<PowBackward0>), grad tensor([0.0013])\n",
      "iteration 38, x tensor([1.5004], requires_grad=True), loss tensor([2.6965e-07], grad_fn=<PowBackward0>), grad tensor([0.0010])\n",
      "iteration 39, x tensor([1.5003], requires_grad=True), loss tensor([1.7259e-07], grad_fn=<PowBackward0>), grad tensor([0.0008])\n",
      "iteration 40, x tensor([1.5003], requires_grad=True), loss tensor([1.1046e-07], grad_fn=<PowBackward0>), grad tensor([0.0007])\n",
      "iteration 41, x tensor([1.5002], requires_grad=True), loss tensor([7.0669e-08], grad_fn=<PowBackward0>), grad tensor([0.0005])\n",
      "iteration 42, x tensor([1.5002], requires_grad=True), loss tensor([4.5228e-08], grad_fn=<PowBackward0>), grad tensor([0.0004])\n",
      "iteration 43, x tensor([1.5001], requires_grad=True), loss tensor([2.8938e-08], grad_fn=<PowBackward0>), grad tensor([0.0003])\n",
      "iteration 44, x tensor([1.5001], requires_grad=True), loss tensor([1.8533e-08], grad_fn=<PowBackward0>), grad tensor([0.0003])\n",
      "iteration 45, x tensor([1.5001], requires_grad=True), loss tensor([1.1872e-08], grad_fn=<PowBackward0>), grad tensor([0.0002])\n",
      "iteration 46, x tensor([1.5001], requires_grad=True), loss tensor([7.5937e-09], grad_fn=<PowBackward0>), grad tensor([0.0002])\n",
      "iteration 47, x tensor([1.5001], requires_grad=True), loss tensor([4.8633e-09], grad_fn=<PowBackward0>), grad tensor([0.0001])\n",
      "iteration 48, x tensor([1.5000], requires_grad=True), loss tensor([3.1125e-09], grad_fn=<PowBackward0>), grad tensor([0.0001])\n",
      "iteration 49, x tensor([1.5000], requires_grad=True), loss tensor([1.9878e-09], grad_fn=<PowBackward0>), grad tensor([8.9169e-05])\n",
      "iteration 50, x tensor([1.5000], requires_grad=True), loss tensor([1.2705e-09], grad_fn=<PowBackward0>), grad tensor([7.1287e-05])\n",
      "iteration 51, x tensor([1.5000], requires_grad=True), loss tensor([8.1174e-10], grad_fn=<PowBackward0>), grad tensor([5.6982e-05])\n",
      "iteration 52, x tensor([1.5000], requires_grad=True), loss tensor([5.1843e-10], grad_fn=<PowBackward0>), grad tensor([4.5538e-05])\n",
      "iteration 53, x tensor([1.5000], requires_grad=True), loss tensor([3.3266e-10], grad_fn=<PowBackward0>), grad tensor([3.6478e-05])\n",
      "iteration 54, x tensor([1.5000], requires_grad=True), loss tensor([2.1151e-10], grad_fn=<PowBackward0>), grad tensor([2.9087e-05])\n",
      "iteration 55, x tensor([1.5000], requires_grad=True), loss tensor([1.3648e-10], grad_fn=<PowBackward0>), grad tensor([2.3365e-05])\n",
      "iteration 56, x tensor([1.5000], requires_grad=True), loss tensor([8.6459e-11], grad_fn=<PowBackward0>), grad tensor([1.8597e-05])\n",
      "iteration 57, x tensor([1.5000], requires_grad=True), loss tensor([5.4627e-11], grad_fn=<PowBackward0>), grad tensor([1.4782e-05])\n",
      "iteration 58, x tensor([1.5000], requires_grad=True), loss tensor([3.5527e-11], grad_fn=<PowBackward0>), grad tensor([1.1921e-05])\n",
      "iteration 59, x tensor([1.5000], requires_grad=True), loss tensor([2.2737e-11], grad_fn=<PowBackward0>), grad tensor([9.5367e-06])\n",
      "iteration 60, x tensor([1.5000], requires_grad=True), loss tensor([1.4552e-11], grad_fn=<PowBackward0>), grad tensor([7.6294e-06])\n",
      "iteration 61, x tensor([1.5000], requires_grad=True), loss tensor([9.6065e-12], grad_fn=<PowBackward0>), grad tensor([6.1989e-06])\n",
      "iteration 62, x tensor([1.5000], requires_grad=True), loss tensor([6.2670e-12], grad_fn=<PowBackward0>), grad tensor([5.0068e-06])\n",
      "iteration 63, x tensor([1.5000], requires_grad=True), loss tensor([4.1069e-12], grad_fn=<PowBackward0>), grad tensor([4.0531e-06])\n",
      "iteration 64, x tensor([1.5000], requires_grad=True), loss tensor([2.7853e-12], grad_fn=<PowBackward0>), grad tensor([3.3379e-06])\n",
      "iteration 65, x tensor([1.5000], requires_grad=True), loss tensor([1.7195e-12], grad_fn=<PowBackward0>), grad tensor([2.6226e-06])\n",
      "iteration 66, x tensor([1.5000], requires_grad=True), loss tensor([1.1511e-12], grad_fn=<PowBackward0>), grad tensor([2.1458e-06])\n",
      "iteration 67, x tensor([1.5000], requires_grad=True), loss tensor([6.9633e-13], grad_fn=<PowBackward0>), grad tensor([1.6689e-06])\n",
      "iteration 68, x tensor([1.5000], requires_grad=True), loss tensor([5.1159e-13], grad_fn=<PowBackward0>), grad tensor([1.4305e-06])\n",
      "iteration 69, x tensor([1.5000], requires_grad=True), loss tensor([3.5527e-13], grad_fn=<PowBackward0>), grad tensor([1.1921e-06])\n",
      "iteration 70, x tensor([1.5000], requires_grad=True), loss tensor([2.2737e-13], grad_fn=<PowBackward0>), grad tensor([9.5367e-07])\n",
      "iteration 71, x tensor([1.5000], requires_grad=True), loss tensor([1.2790e-13], grad_fn=<PowBackward0>), grad tensor([7.1526e-07])\n",
      "iteration 72, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 73, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 74, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 75, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 76, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 77, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 78, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 79, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 80, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 81, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 82, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 83, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 84, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 85, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 86, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 87, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 88, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 89, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 90, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 91, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 92, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 93, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 94, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 95, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 96, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 97, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 98, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n",
      "iteration 99, x tensor([1.5000], requires_grad=True), loss tensor([5.6843e-14], grad_fn=<PowBackward0>), grad tensor([4.7684e-07])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(x):\n",
    "    return (x - 1.5)**2\n",
    "\n",
    "x = torch.tensor([4], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([x], lr=0.1) # SGD с коэффициентом скорости обучения 0.1\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    y = f(x)\n",
    "    optimizer.zero_grad() # сбрасываем градиенты перед вычислением новых\n",
    "    y.backward()          # вычисляем градиенты функции потерь f(x) относительно x\n",
    "    optimizer.step()      # выполняем шаг оптимизации, обновив значение x в направлении, которое уменьшает значение f(x)\n",
    "    print(f'iteration {i}, x {x}, loss {y}, grad {x.grad}')\n",
    "    #     with torch.no_grad():\n",
    "    #         x -= 0.1 * x.grad\n",
    "    #x.grad.zero_()\n",
    "    #print(f'grad {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.114003132Z",
     "start_time": "2023-10-06T08:13:04.051808049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 99, x 4.0, loss 6.25, grad 5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.5000, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''В предоставленном коде происходит оптимизация параметра x для минимизации \n",
    "функции потерь f(x) = (x - 1.5)^2 вручную, без использования оптимизатора PyTorch.'''\n",
    "\n",
    "x = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(f'iteration {i}, x {x}, loss {y}, grad {x.grad}')\n",
    "#with torch.no_grad():\n",
    "x = x - 0.1 * x.grad\n",
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:13:07.115884538Z",
     "start_time": "2023-10-06T08:13:04.078943566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5000, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
