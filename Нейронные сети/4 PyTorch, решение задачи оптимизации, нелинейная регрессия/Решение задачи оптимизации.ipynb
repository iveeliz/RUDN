{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи оптимизации\n",
    "\n",
    "Решите простую задачу безусловной оптимизации в двумерном пространстве:  \n",
    "$$f(\\boldsymbol x) = -8x_1 - 16x_2 + x_1^2 + 4x_2^2$$\n",
    "Используя два метода:\n",
    " - аналитически (функция квадратичная, выпуклая)\n",
    " - методом градиентного спуска, используя один из методов оптимизации torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аналитически \n",
    "\n",
    "$\\nabla f(x) = \\big(\\frac{\\partial f(x)}{\\partial x_1}, \\frac{\\partial f(x)}{\\partial x_2} \\big) = (-8 + 2x_1, -16 + 8x_2)$\n",
    "\n",
    "$\\frac{\\partial f(x)}{\\partial x_1} = -8 + 2x_1 = 0 \\implies x_1 = 4$\n",
    "\n",
    "$\\frac{\\partial f(x)}{\\partial x_2} = -16 + 8x_2 = 0 \\implies x_2 = 2$\n",
    "\n",
    "$H = \\nabla^2 f(x) =\n",
    "\\begin{pmatrix} \n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2f}{\\partial x_1 \\partial x_2}\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2f}{\\partial x_2^2}\\\\\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix} \n",
    "2 & 0\\\\\n",
    "0 & 8\\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "$H$ положительно определена $\\implies f^* = f(4, 2) = -32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return -8 * x1 - 16 * x2 + x1 ** 2 + 4 * x2 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 = 3.0, x2 = 3.0, loss = -27.0, grad x1 = -2.0, grad x2 = 8.0\n",
      "iteration 5, x1 = 3.4095099, x2 = 2.07776, loss = -31.6271381, grad x1 = -1.1809802, grad x2 = 0.6220798\n",
      "iteration 10, x1 = 3.6513214, x2 = 2.0060465, loss = -31.8782749, grad x1 = -0.6973572, grad x2 = 0.0483723\n",
      "iteration 15, x1 = 3.7941089, x2 = 2.0004702, loss = -31.9576111, grad x1 = -0.4117823, grad x2 = 0.0037613\n",
      "iteration 20, x1 = 3.8784232, x2 = 2.0000367, loss = -31.985218, grad x1 = -0.2431536, grad x2 = 0.0002937\n",
      "iteration 25, x1 = 3.9282103, x2 = 2.0000029, loss = -31.9948463, grad x1 = -0.1435795, grad x2 = 2.29e-05\n",
      "iteration 30, x1 = 3.9576092, x2 = 2.0000002, loss = -31.9982033, grad x1 = -0.0847816, grad x2 = 1.9e-06\n",
      "iteration 35, x1 = 3.9749687, x2 = 2.0000002, loss = -31.9993706, grad x1 = -0.0500627, grad x2 = 1.9e-06\n",
      "iteration 40, x1 = 3.9852192, x2 = 2.0000002, loss = -31.9997826, grad x1 = -0.0295615, grad x2 = 1.9e-06\n",
      "iteration 45, x1 = 3.991272, x2 = 2.0000002, loss = -31.9999237, grad x1 = -0.0174561, grad x2 = 1.9e-06\n",
      "iteration 50, x1 = 3.9948463, x2 = 2.0000002, loss = -31.9999733, grad x1 = -0.0103073, grad x2 = 1.9e-06\n",
      "iteration 55, x1 = 3.9969568, x2 = 2.0000002, loss = -31.9999886, grad x1 = -0.0060863, grad x2 = 1.9e-06\n",
      "iteration 60, x1 = 3.998203, x2 = 2.0000002, loss = -31.9999962, grad x1 = -0.0035939, grad x2 = 1.9e-06\n",
      "iteration 65, x1 = 3.998939, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0021219, grad x2 = 1.9e-06\n",
      "iteration 70, x1 = 3.9993734, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0012531, grad x2 = 1.9e-06\n",
      "iteration 75, x1 = 3.99963, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0007401, grad x2 = 1.9e-06\n",
      "iteration 80, x1 = 3.9997816, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0004368, grad x2 = 1.9e-06\n",
      "iteration 85, x1 = 3.999871, x2 = 2.0000002, loss = -31.9999962, grad x1 = -0.000258, grad x2 = 1.9e-06\n",
      "iteration 90, x1 = 3.9999237, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0001526, grad x2 = 1.9e-06\n",
      "iteration 95, x1 = 3.9999549, x2 = 2.0000002, loss = -31.9999962, grad x1 = -9.01e-05, grad x2 = 1.9e-06\n",
      "iteration 100, x1 = 3.9999733, x2 = 2.0000002, loss = -32.0, grad x1 = -5.34e-05, grad x2 = 1.9e-06\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(3., dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(3., dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "for i in range(101):\n",
    "    \n",
    "    # Вычисление значения функции для текущих значений x1 и x2\n",
    "    y = f(x1, x2)\n",
    "    \n",
    "    # Вычисление градиентов функции относительно x1 и x2\n",
    "    y.backward()\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f'iteration {i}, x1 = {round(x1.item(), 7)}, x2 = {round(x2.item(), 7)}, '\n",
    "              f'loss = {round(y.item(), 7)}, grad x1 = {round(x1.grad.item(), 7)}, grad x2 = {round(x2.grad.item(), 7)}')\n",
    "        \n",
    "    # Временно отключаем вычисление градиентов, чтобы обновить x1 и x2\n",
    "    with torch.no_grad():\n",
    "        # Обновление x1 и x2 с использованием градиентного спуска\n",
    "        x1 -= 0.05 * x1.grad\n",
    "        x2 -= 0.05 * x2.grad\n",
    "    \n",
    "    # Обнуление градиентов x1 и x2, готовим их для следующей итерации\n",
    "    x1.grad.zero_()\n",
    "    x2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f* = -32.0, x1 = 3.9999759, x2 = 2.0000002\n"
     ]
    }
   ],
   "source": [
    "print(f'f* = {round(y.item(), 7)}, x1 = {round(x1.item(), 7)}, x2 = {round(x2.item(), 7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Используем torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 = 3.0999999, x2 = 2.5999999, loss = -27.0, grad x1 = -2.0, grad x2 = 8.0\n",
      "iteration 5, x1 = 3.4685588, x2 = 2.0466559, loss = -31.6271381, grad x1 = -1.1809802, grad x2 = 0.6220798\n",
      "iteration 10, x1 = 3.6861892, x2 = 2.003628, loss = -31.8782749, grad x1 = -0.6973572, grad x2 = 0.0483723\n",
      "iteration 15, x1 = 3.814698, x2 = 2.000282, loss = -31.9576111, grad x1 = -0.4117823, grad x2 = 0.0037613\n",
      "iteration 20, x1 = 3.8905809, x2 = 2.0000219, loss = -31.985218, grad x1 = -0.2431536, grad x2 = 0.0002937\n",
      "iteration 25, x1 = 3.9353893, x2 = 2.0000017, loss = -31.9948463, grad x1 = -0.1435795, grad x2 = 2.29e-05\n",
      "iteration 30, x1 = 3.9618483, x2 = 2.0000002, loss = -31.9982033, grad x1 = -0.0847816, grad x2 = 1.9e-06\n",
      "iteration 35, x1 = 3.9774718, x2 = 2.0000002, loss = -31.9993706, grad x1 = -0.0500627, grad x2 = 1.9e-06\n",
      "iteration 40, x1 = 3.9866972, x2 = 2.0000002, loss = -31.9997826, grad x1 = -0.0295615, grad x2 = 1.9e-06\n",
      "iteration 45, x1 = 3.9921448, x2 = 2.0000002, loss = -31.9999237, grad x1 = -0.0174561, grad x2 = 1.9e-06\n",
      "iteration 50, x1 = 3.9953618, x2 = 2.0000002, loss = -31.9999733, grad x1 = -0.0103073, grad x2 = 1.9e-06\n",
      "iteration 55, x1 = 3.997261, x2 = 2.0000002, loss = -31.9999886, grad x1 = -0.0060863, grad x2 = 1.9e-06\n",
      "iteration 60, x1 = 3.9983828, x2 = 2.0000002, loss = -31.9999962, grad x1 = -0.0035939, grad x2 = 1.9e-06\n",
      "iteration 65, x1 = 3.9990451, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0021219, grad x2 = 1.9e-06\n",
      "iteration 70, x1 = 3.9994361, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0012531, grad x2 = 1.9e-06\n",
      "iteration 75, x1 = 3.9996669, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0007401, grad x2 = 1.9e-06\n",
      "iteration 80, x1 = 3.9998035, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0004368, grad x2 = 1.9e-06\n",
      "iteration 85, x1 = 3.9998839, x2 = 2.0000002, loss = -31.9999962, grad x1 = -0.000258, grad x2 = 1.9e-06\n",
      "iteration 90, x1 = 3.9999313, x2 = 2.0000002, loss = -32.0, grad x1 = -0.0001526, grad x2 = 1.9e-06\n",
      "iteration 95, x1 = 3.9999595, x2 = 2.0000002, loss = -31.9999962, grad x1 = -9.01e-05, grad x2 = 1.9e-06\n",
      "iteration 100, x1 = 3.9999759, x2 = 2.0000002, loss = -32.0, grad x1 = -5.34e-05, grad x2 = 1.9e-06\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(3., dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(3., dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "optimizer  = torch.optim.SGD([x1, x2], lr=0.05)\n",
    "\n",
    "for i in range(101):\n",
    "    \n",
    "    # Вычисление значения функции для текущих значений x1 и x2\n",
    "    y = f(x1, x2)\n",
    "    \n",
    "    # Обнуление градиентов для предотвращения их накопления\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Вычисление градиентов функции относительно x1 и x2\n",
    "    y.backward()\n",
    "    \n",
    "    # Выполнение одного шага оптимизации с использованием градиентов и learning rate\n",
    "    optimizer.step() \n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f'iteration {i}, x1 = {round(x1.item(), 7)}, x2 = {round(x2.item(), 7)}, '\n",
    "              f'loss = {round(y.item(), 7)}, grad x1 = {round(x1.grad.item(), 7)}, grad x2 = {round(x2.grad.item(), 7)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f* = -32.0, x1 = 3.9999759, x2 = 2.0000002\n"
     ]
    }
   ],
   "source": [
    "print(f'f* = {round(y.item(), 7)}, x1 = {round(x1.item(), 7)}, x2 = {round(x2.item(), 7)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
